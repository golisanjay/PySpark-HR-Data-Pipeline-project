{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Data Analysis using Spark\n",
    "### Scenario\n",
    "You have been tasked by the HR department of a company to create a data pipeline that can take in employee data in a CSV format. Your responsibilities include analyzing the data, applying any required transformations, and facilitating the extraction of valuable insights from the processed data.\n",
    "HR Employee Data Pipeline\n",
    "# Author: Gael Mayanza-Ouamba\n",
    "# Description:\n",
    "# ETL pipeline using PySpark to process employee CSV data,\n",
    "# perform transformations, aggregations, and analytics.\n",
    "Given your role as a data engineer, you've been requested to leverage Apache Spark components to accomplish the tasks.\n",
    "\n",
    "- Task 1: Generate DataFrame from CSV data.\n",
    "- Task 2: Define a schema for the data.\n",
    "- Task 3: Display schema of DataFrame.\n",
    "- Task 4: Create a temporary view.\n",
    "- Task 5: Execute an SQL query.\n",
    "- Task 6: Calculate Average Salary by Department.\n",
    "- Task 7: Filter and Display IT Department Employees.\n",
    "- Task 8: Add 10% Bonus to Salaries.\n",
    "- Task 9: Find Maximum Salary by Age.\n",
    "- Task 10: Self-Join on Employee Data.\n",
    "- Task 11: Calculate Average Employee Age.\n",
    "- Task 12: Calculate Total Salary by Department.\n",
    "- Task 13: Sort Data by Age and Salary.\n",
    "- Task 14: Count Employees in Each Department.\n",
    "- Task 15: Filter Employees with the letter o in the Name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites \n",
    "\n",
    "1. For this project, will be using Python and Spark (PySpark). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.4.tar.gz (311.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: wget in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (3.2)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.4-py2.py3-none-any.whl size=311905466 sha256=3905bd816a3353e9d73c28e6798cb059a849667776c016d2e6c83ec9fb3571a3\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/4e/66/db/939eb1c49afb8a7fd2c4e393ad34e12b77db67bb4cc974c00e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, findspark, pyspark\n",
      "Successfully installed findspark-2.0.1 py4j-0.10.9.7 pyspark-3.4.4\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages \n",
    "\n",
    "!pip install pyspark  findspark wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the SparkContext.   \n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 18:12:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkContext object  \n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a SparkSession  \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download the CSV data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employees.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the CSV data first into a local `employees.csv` file\n",
    "import wget\n",
    "wget.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Generate a Spark DataFrame from the CSV data\n",
    "\n",
    "Read data from the provided CSV file, `employees.csv` and import it into a Spark DataFrame variable named `employees_df`.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|\n",
      "+------+---------+------+---+----------+\n",
      "|   198|   Donald|  2600| 29|        IT|\n",
      "|   199|  Douglas|  2600| 34|     Sales|\n",
      "|   200| Jennifer|  4400| 36| Marketing|\n",
      "|   201|  Michael| 13000| 32|        IT|\n",
      "|   202|      Pat|  6000| 39|        HR|\n",
      "|   203|    Susan|  6500| 36| Marketing|\n",
      "|   204|  Hermann| 10000| 29|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|\n",
      "|   206|  William|  8300| 37|        IT|\n",
      "|   100|   Steven| 24000| 39|        IT|\n",
      "|   101|    Neena| 17000| 27|     Sales|\n",
      "|   102|      Lex| 17000| 37| Marketing|\n",
      "|   103|Alexander|  9000| 39| Marketing|\n",
      "|   104|    Bruce|  6000| 38|        IT|\n",
      "|   105|    David|  4800| 39|        IT|\n",
      "|   106|    Valli|  4800| 38|     Sales|\n",
      "|   107|    Diana|  4200| 35|     Sales|\n",
      "|   108|    Nancy| 12008| 28|     Sales|\n",
      "|   109|   Daniel|  9000| 35|        HR|\n",
      "|   110|     John|  8200| 31| Marketing|\n",
      "+------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from the \"emp\" CSV file and import it into a DataFrame variable named \"employees_df\"  \n",
    "employees_df = spark.read.csv('employees.csv', header = True, inferSchema = True)\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Define a schema for the data\n",
    "\n",
    "Construct a schema for the input data and then utilize the defined schema to read the CSV file to create a DataFrame named `employees_df`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Schema for the input data and read the file using the user-defined Schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "# Define the schema structure\n",
    "employee_schema = StructType([\n",
    "    StructField('Emp_No', IntegerType(), True),\n",
    "    StructField('Emp_Name', StringType(), True),\n",
    "    StructField('Salary', DoubleType(), True),\n",
    "    StructField('Age', IntegerType(), True),\n",
    "    StructField('Department', StringType(), True)\n",
    "])\n",
    "# Read the CSV file using the defined schema\n",
    "employees_df = spark.read.csv('employees.csv', header = True, schema = employee_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Display schema of DataFrame\n",
    "\n",
    "Display the schema of the `employees_df` DataFrame, showing all columns and their respective data types.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp_No: integer (nullable = true)\n",
      " |-- Emp_Name: string (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all columns of the DataFrame, along with their respective data types\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Create a temporary view\n",
    "\n",
    "Create a temporary view named `employees` for the `employees_df` DataFrame, enabling Spark SQL queries on the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view named \"employees\" for the DataFrame\n",
    "employees_df.createOrReplaceTempView('employees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Execute an SQL query\n",
    "\n",
    "Compose and execute an SQL query to fetch the records from the `employees` view where the age of employees exceeds 30. Then, display the result of the SQL query, showcasing the filtered records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   200|   Jennifer|  4400| 36| Marketing|\n",
      "|   201|    Michael| 13000| 32|        IT|\n",
      "|   202|        Pat|  6000| 39|        HR|\n",
      "|   203|      Susan|  6500| 36| Marketing|\n",
      "|   205|    Shelley| 12008| 33|   Finance|\n",
      "|   206|    William|  8300| 37|        IT|\n",
      "|   100|     Steven| 24000| 39|        IT|\n",
      "|   102|        Lex| 17000| 37| Marketing|\n",
      "|   103|  Alexander|  9000| 39| Marketing|\n",
      "|   104|      Bruce|  6000| 38|        IT|\n",
      "|   105|      David|  4800| 39|        IT|\n",
      "|   106|      Valli|  4800| 38|     Sales|\n",
      "|   107|      Diana|  4200| 35|     Sales|\n",
      "|   109|     Daniel|  9000| 35|        HR|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   111|     Ismael|  7700| 32|        IT|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   113|       Luis|  6900| 34|     Sales|\n",
      "|   116|     Shelli|  2900| 37|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query to fetch solely the records from the View where the age exceeds 30\n",
    "age_over_30 = spark.sql(\"SELECT * FROM employees WHERE  age > 30 \")\n",
    "# Display the filtered records\n",
    "age_over_30.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Calculate Average Salary by Department\n",
    "\n",
    "Compose an SQL query to retrieve the average salary of employees grouped by department. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:===================================>                    (47 + 8) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|Department|        avg_s_emp|\n",
      "+----------+-----------------+\n",
      "|     Sales|5492.923076923077|\n",
      "|        HR|           5837.5|\n",
      "|   Finance|           5730.8|\n",
      "| Marketing|6633.333333333333|\n",
      "|        IT|           7400.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SQL query to calculate the average salary of employees grouped by department\n",
    "avg_salary = spark.sql(\"SELECT Department, AVG(Salary) as avg_s_emp  FROM employees  GROUP BY Department \")\n",
    "# Display the filtered records\n",
    "avg_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: Filter and Display IT Department Employees\n",
    "\n",
    "Apply a filter on the `employees_df` DataFrame to select records where the department is `'IT'`. Display the filtered DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----------+\n",
      "|Emp_No|Emp_Name|Salary|Age|Department|\n",
      "+------+--------+------+---+----------+\n",
      "|   198|  Donald|  2600| 29|        IT|\n",
      "|   201| Michael| 13000| 32|        IT|\n",
      "|   206| William|  8300| 37|        IT|\n",
      "|   100|  Steven| 24000| 39|        IT|\n",
      "|   104|   Bruce|  6000| 38|        IT|\n",
      "|   105|   David|  4800| 39|        IT|\n",
      "|   111|  Ismael|  7700| 32|        IT|\n",
      "|   129|   Laura|  3300| 38|        IT|\n",
      "|   132|      TJ|  2100| 34|        IT|\n",
      "|   136|   Hazel|  2200| 29|        IT|\n",
      "+------+--------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a filter to select records where the department is 'IT'\n",
    "filtered_it_dep = employees_df.filter(employees_df.Department == 'IT')\n",
    "# Display the filtered records\n",
    "filtered_it_dep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----------+\n",
      "|Emp_No|Emp_Name|Salary|Age|Department|\n",
      "+------+--------+------+---+----------+\n",
      "|   198|  Donald|  2600| 29|        IT|\n",
      "|   201| Michael| 13000| 32|        IT|\n",
      "|   206| William|  8300| 37|        IT|\n",
      "|   100|  Steven| 24000| 39|        IT|\n",
      "|   104|   Bruce|  6000| 38|        IT|\n",
      "|   105|   David|  4800| 39|        IT|\n",
      "|   111|  Ismael|  7700| 32|        IT|\n",
      "|   129|   Laura|  3300| 38|        IT|\n",
      "|   132|      TJ|  2100| 34|        IT|\n",
      "|   136|   Hazel|  2200| 29|        IT|\n",
      "+------+--------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_it_dep = employees_df.filter(\"Department = 'IT'\")\n",
    "filtered_it_dep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8: Add 10% Bonus to Salaries\n",
    "\n",
    "Perform a transformation to add a new column named \"SalaryAfterBonus\" to the DataFrame. Calculate the new salary by adding a 10% bonus to each employee's salary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+------------------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|  SalaryAfterBonus|\n",
      "+------+---------+------+---+----------+------------------+\n",
      "|   198|   Donald|  2600| 29|        IT|2860.0000000000005|\n",
      "|   199|  Douglas|  2600| 34|     Sales|2860.0000000000005|\n",
      "|   200| Jennifer|  4400| 36| Marketing|            4840.0|\n",
      "|   201|  Michael| 13000| 32|        IT|14300.000000000002|\n",
      "|   202|      Pat|  6000| 39|        HR| 6600.000000000001|\n",
      "|   203|    Susan|  6500| 36| Marketing| 7150.000000000001|\n",
      "|   204|  Hermann| 10000| 29|   Finance|           11000.0|\n",
      "|   205|  Shelley| 12008| 33|   Finance|13208.800000000001|\n",
      "|   206|  William|  8300| 37|        IT|            9130.0|\n",
      "|   100|   Steven| 24000| 39|        IT|26400.000000000004|\n",
      "|   101|    Neena| 17000| 27|     Sales|           18700.0|\n",
      "|   102|      Lex| 17000| 37| Marketing|           18700.0|\n",
      "|   103|Alexander|  9000| 39| Marketing|            9900.0|\n",
      "|   104|    Bruce|  6000| 38|        IT| 6600.000000000001|\n",
      "|   105|    David|  4800| 39|        IT|            5280.0|\n",
      "|   106|    Valli|  4800| 38|     Sales|            5280.0|\n",
      "|   107|    Diana|  4200| 35|     Sales|            4620.0|\n",
      "|   108|    Nancy| 12008| 28|     Sales|13208.800000000001|\n",
      "|   109|   Daniel|  9000| 35|        HR|            9900.0|\n",
      "|   110|     John|  8200| 31| Marketing|            9020.0|\n",
      "+------+---------+------+---+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add a new column \"SalaryAfterBonus\" with 10% bonus added to the original salary\n",
    "employees_df2  = employees_df.withColumn(\"SalaryAfterBonus\", col('Salary') * 1.1)\n",
    "# Display the transformed records\n",
    "employees_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9: Find Maximum Salary by Age\n",
    "\n",
    "Group the data by age and calculate the maximum salary for each age group. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|age|maximum_salary|\n",
      "+---+--------------+\n",
      "| 31|          8200|\n",
      "| 34|          7800|\n",
      "| 28|         12008|\n",
      "| 27|         17000|\n",
      "| 26|          3600|\n",
      "| 37|         17000|\n",
      "| 35|          9000|\n",
      "| 39|         24000|\n",
      "| 38|          6000|\n",
      "| 29|         10000|\n",
      "| 32|         13000|\n",
      "| 33|         12008|\n",
      "| 30|          8000|\n",
      "| 36|          7900|\n",
      "+---+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Group data by age and calculate the maximum salary for each age group\n",
    "max_sal = employees_df.groupBy('age').agg(max('Salary').alias('maximum_salary'))\n",
    "\n",
    "# Display the maximum salary for each age group\n",
    "max_sal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10: Self-Join on Employee Data\n",
    "\n",
    "Join the \"employees_df\" DataFrame with itself based on the \"Emp_No\" column. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department| Emp_Name|Salary|Age|Department|\n",
      "+------+---------+------+---+----------+---------+------+---+----------+\n",
      "|   198|   Donald|  2600| 29|        IT|   Donald|  2600| 29|        IT|\n",
      "|   199|  Douglas|  2600| 34|     Sales|  Douglas|  2600| 34|     Sales|\n",
      "|   200| Jennifer|  4400| 36| Marketing| Jennifer|  4400| 36| Marketing|\n",
      "|   201|  Michael| 13000| 32|        IT|  Michael| 13000| 32|        IT|\n",
      "|   202|      Pat|  6000| 39|        HR|      Pat|  6000| 39|        HR|\n",
      "|   203|    Susan|  6500| 36| Marketing|    Susan|  6500| 36| Marketing|\n",
      "|   204|  Hermann| 10000| 29|   Finance|  Hermann| 10000| 29|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|  Shelley| 12008| 33|   Finance|\n",
      "|   206|  William|  8300| 37|        IT|  William|  8300| 37|        IT|\n",
      "|   100|   Steven| 24000| 39|        IT|   Steven| 24000| 39|        IT|\n",
      "|   101|    Neena| 17000| 27|     Sales|    Neena| 17000| 27|     Sales|\n",
      "|   102|      Lex| 17000| 37| Marketing|      Lex| 17000| 37| Marketing|\n",
      "|   103|Alexander|  9000| 39| Marketing|Alexander|  9000| 39| Marketing|\n",
      "|   104|    Bruce|  6000| 38|        IT|    Bruce|  6000| 38|        IT|\n",
      "|   105|    David|  4800| 39|        IT|    David|  4800| 39|        IT|\n",
      "|   106|    Valli|  4800| 38|     Sales|    Valli|  4800| 38|     Sales|\n",
      "|   107|    Diana|  4200| 35|     Sales|    Diana|  4200| 35|     Sales|\n",
      "|   108|    Nancy| 12008| 28|     Sales|    Nancy| 12008| 28|     Sales|\n",
      "|   109|   Daniel|  9000| 35|        HR|   Daniel|  9000| 35|        HR|\n",
      "|   110|     John|  8200| 31| Marketing|     John|  8200| 31| Marketing|\n",
      "+------+---------+------+---+----------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the DataFrame with itself based on the \"Emp_No\" column\n",
    "emp_self_join = employees_df.alias('e1').join(employees_df.alias('e2'), on = 'Emp_No', how = 'inner')\n",
    "emp_self_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 11: Calculate Average Employee Age\n",
    "\n",
    "Calculate the average age of employees using the built-in aggregation function. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|average_age|\n",
      "+-----------+\n",
      "|      33.56|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average age of employees\n",
    "from pyspark.sql.functions import avg \n",
    "avg_age_emp = employees_df.agg(avg('Age').alias('average_age'))\n",
    "avg_age_emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 12: Calculate Total Salary by Department\n",
    "\n",
    "Calculate the total salary for each department using the built-in aggregation function. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|Department|total_salary|\n",
      "+----------+------------+\n",
      "|     Sales|       71408|\n",
      "|        HR|       46700|\n",
      "|   Finance|       57308|\n",
      "| Marketing|       59700|\n",
      "|        IT|       74000|\n",
      "+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the total salary for each department. Hint - User GroupBy and Aggregate functions\n",
    "from pyspark.sql.functions import sum \n",
    "sal_by_dep = employees_df.groupBy('Department').agg(sum('Salary').alias('total_salary'))\n",
    "\n",
    "sal_by_dep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 13: Sort Data by Age and Salary\n",
    "\n",
    "Apply a transformation to sort the DataFrame by age in ascending order and then by salary in descending order. Display the sorted DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|\n",
      "+------+---------+------+---+----------+\n",
      "|   137|   Renske|  3600| 26| Marketing|\n",
      "|   101|    Neena| 17000| 27|     Sales|\n",
      "|   114|      Den| 11000| 27|   Finance|\n",
      "|   108|    Nancy| 12008| 28|     Sales|\n",
      "|   130|    Mozhe|  2800| 28| Marketing|\n",
      "|   126|    Irene|  2700| 28|        HR|\n",
      "|   204|  Hermann| 10000| 29|   Finance|\n",
      "|   115|Alexander|  3100| 29|   Finance|\n",
      "|   134|  Michael|  2900| 29|     Sales|\n",
      "|   198|   Donald|  2600| 29|        IT|\n",
      "|   140|   Joshua|  2500| 29|   Finance|\n",
      "|   136|    Hazel|  2200| 29|        IT|\n",
      "|   120|  Matthew|  8000| 30|        HR|\n",
      "|   110|     John|  8200| 31| Marketing|\n",
      "|   127|    James|  2400| 31|        HR|\n",
      "|   201|  Michael| 13000| 32|        IT|\n",
      "|   111|   Ismael|  7700| 32|        IT|\n",
      "|   119|    Karen|  2500| 32|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|\n",
      "|   124|    Kevin|  5800| 33| Marketing|\n",
      "+------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by age in ascending order and then by salary in descending order\n",
    "sorted_df = employees_df.orderBy(['Age','Salary'], ascending = [True, False])\n",
    "\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 14: Count Employees in Each Department\n",
    "\n",
    "Calculate the number of employees in each department. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|Department|num_of_emp|\n",
      "+----------+----------+\n",
      "|     Sales|        13|\n",
      "|        HR|         8|\n",
      "|   Finance|        10|\n",
      "| Marketing|         9|\n",
      "|        IT|        10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Calculate the number of employees in each department\n",
    "emp_dep = employees_df.groupBy('Department').agg(count('Emp_No').alias('num_of_emp'))\n",
    "emp_dep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 15: Filter Employees with the letter o in the Name\n",
    "\n",
    "Apply a filter to select records where the employee's name contains the letter `'o'`. Display the filtered DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   198|     Donald|  2600| 29|        IT|\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   130|      Mozhe|  2800| 28| Marketing|\n",
      "|   133|      Jason|  3300| 38|     Sales|\n",
      "|   139|       John|  2700| 36|     Sales|\n",
      "|   140|     Joshua|  2500| 29|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a filter to select records where the employee's name contains the letter 'o'\n",
    "emp_with_o = employees_df.filter(col('Emp_Name').contains('o'))\n",
    "\n",
    "emp_with_o.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Change Log -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "prev_pub_hash": "aff0a6b4ec3a9cf15ae5d70a5c7ecac07e8a7f43b412a755232c9c99d1062fc8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
